\documentclass[12pt]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{tikz}

% make it pretty
\input{../header.tex}

%------------------------------------------------

\title{Singular Value Decomposition (SVD)}
\subtitle{Matrix Algebra 4 Statistical Learning}
\author{\href{http://www.gastonsanchez.com}{Gaston Sanchez}}
\institute{\href{https://creativecommons.org/licenses/by-sa/4.0/}{\tt \scriptsize \color{foreground} CC BY-SA 4.0}}
\date{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}



% no page number in first slide
{
  \setbeamertemplate{footline}{} 
  \frame{\titlepage} 
}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{Introduction}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Two Special Decompositions}

Last time we talked about the Eigen Value Decomposition (EVD).

\bigskip
In these slides, we'll talk about a closely related decomposition of EVD:
the so-called {\hilit Singular Value Decomposition (SVD)}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{Recap}

Matrix decompositions, also known as matrix factorizations

{\Large
$$
\mathbf{M = A B} \qquad \text{or} \qquad \mathbf{M = A B C}
$$
}

are a means of expressing a matrix as a product 
of usually two or three {\hilit simpler} matrices.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Types of matrices}

\bb{Two types of matrices}
We said that in data analysis we typically concentrate on two types of matrices:
\eb

\bbi
 \item general \textbf{rectangular} matrices used to represent data tables.
 \item \textbf{positive semi-definite} matrices used to represent covariance matrices, 
correlation matrices, and any matrix that results from a crossproduct.
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD}

\bb{Singular Value Decomposition}
\bbi
  \item One of the most important decompositions in matrix algebra
  \item Can be applied to {\hilit \textbf{any}} rectangular matrix
  \item ANY: rectangular or square, singular or nonsigular.
\ei
\eb

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Singular Value Decomposition}

Let $\mathbf{M}$ be an $n \times p$ matrix.

\bigskip
For convenience, let's also assume that
$\mathbf{M}$ is a "tall" matrix with $n > p$, although this is not essential.

\bigskip
Likewise, let's assume for the moment that the rank of $\mathbf{M}$ is $r \leq p < n$

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Singular Value Decomposition}

We can represent $\mathbf{M}$ as a triple product given by:

$$ 
\mathbf{M = U D V^\mathsf{T}}
$$

where
\bi
 \item $\mathbf{U}$ is a $n \times r$ matrix which is \textbf{orthonormal} by columns:
 $\mathbf{U^\mathsf{T} U} = \mathbf{I}_r$
 \item $\mathbf{D}$ is a $r \times r$ \textbf{diagonal} matrix consisting of $r$ positive diagonal entries.
 \item $\mathbf{V}$ is a $p \times r$ matrix which is \textbf{orthonormal} by columns:
 $\mathbf{V^\mathsf{T} V} = \mathbf{I}_r$
\ei

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Singular Value Decomposition}

The representation of $\mathbf{M}$ as the triple product $\mathbf{U D V^\mathsf{T}}$
is called its {\mdlit \textit{singular value decomposition}}, occasionally
referred to as ``basic structure''.

\bigskip

\bbi
 \item $\mathbf{U}$ is the matrix of \textbf{left singular vectors} of $\mathbf{M}$
 \item $\mathbf{D}$ is the matrix of \textbf{singular values} of $\mathbf{M}$
 \item $\mathbf{V}$ is the matrix of \textbf{right singular vectors} of $\mathbf{M}$
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD}

$$
\mathbf{M = U D V^\mathsf{T}}
$$

$$
\mathbf{M} = 
\
\begin{bmatrix} 
u_{11} & \cdots & u_{1r} \\
u_{21} & \cdots & u_{2r} \\
\vdots & \ddots & \vdots \\
u_{n1} & \cdots & u_{nr} \\
\end{bmatrix}
\
\begin{bmatrix} 
l_1 & \cdots & 0 \\ 
\vdots & \ddots & \vdots \\
0 & \cdots & l_r \\
\end{bmatrix}
\
\begin{bmatrix} 
v_{11} & \cdots & v_{p1} \\
\vdots & \ddots & \vdots \\
v_{1r} & \cdots & v_{pr} \\
\end{bmatrix}
$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Diagram}
\begin{center}
\ig[width=10cm]{images/svd-decomposition2.pdf}

{\lolit When $\mathbf{M}$ is of rank $r < p$}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Diagram}
\begin{center}
\ig[width=10cm]{images/svd-decomposition1.pdf}

{\lolit When $\mathbf{M}$ is of full rank $p$}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{About SVD}

\bb{Singular Value Decomposition}
We can think of the SVD structure as \textit{the basic structure of a matrix}. 
What do we mean by ``basic''? Well, this has to do with what each of the matrices 
$\mathbf{U D V^\mathsf{T}}$ represent.
\eb

\bi
  \item $\mathbf{U}$ is the orthonormalized matrix which is the most basic component. It's like the skeleton of the matrix.
  \item $\mathbf{D}$ is referred to as the \textit{spectrum} and it is a scale component.
  \item $\mathbf{V}$ is an orientation component, also referred to as
  the \textit{rotation} matrix.
\ei

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{About SVD}

Note that: 

\bbi
 \item $\mathbf{U}$ cannot be orthogonal ($\mathbf{U U^\mathsf{T} = I_n}$) unless
$r = p$
 \item $\mathbf{V}$ cannot be orthogonal ($\mathbf{V V^\mathsf{T} = I_p}$) unless $r = p = m$
 \item All elements of $\mathbf{D}$ can be taken to be positive, 
 ordered from large to small (with ties allowed)
\ei

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Full SVD}

If we let $\mathbf{D}$ be the first $r$ rows and $r$ columns embedded in a (larger)
$n \times p$ \textit{rectangular} matrix, with $n - r$ rows and $p - r$ columns of zeros
elsewhere, both $\mathbf{U}$ and $\mathbf{V^\mathsf{T}}$ could be made fully orthogonal and,
in this sense, properly constitute rotation matrices of order $n \times n$ and $p \times p$,
respectively. This generalization can be called the {\hilit full SVD} of a matrix.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{``Full'' SVD Diagram}
\begin{center}
\ig[width=10cm]{images/svd-decomposition3.pdf}

{\lolit When $\mathbf{M}$ is of rank $r \leq p$}
\end{center}
\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{About SVD}

The preceding generalization is a significant one. It tells us that ANY matrix
with real-valued entries can be represented as the product of:

\bbi
 \item a rotation (possibly followed by a reflection), followed by
 \item a stretch, followed by
 \item a rotation
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD}

\bbi
  \item $\mathbf{U}$ is unitary, and its columns form a basis for the space spanned by the columns of $\mathbf{M}$. \\
  $$
  \mathbf{U^\mathsf{T} U} = \mathbf{I}_{p}
  $$
  \item $\mathbf{V}$ is unitary, and its columns form a basis for the space spanned by the rows of $\mathbf{M}$. \\
  $$
  \mathbf{V^\mathsf{T} V} = \mathbf{I}_{p}
  $$
  \item $\mathbf{D}$ has non-negative real numbers on the diagonal (assuming $\mathbf{M}$ is real).
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{SVD in R}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{\texttt{svd()} in R}
\begin{block}{\texttt{svd()} function}
R provides the function {\hilit \code{svd()}} to perform a 
singular value decomposition of a given matrix
\end{block}

\bb{\texttt{svd()} output}
A list with the following components
\bi
 \item[\code{d}] a vector containing the singular values
 \item[\code{u}] a matrix whose columns contain the left singular vectors
 \item[\code{v}] a matrix whose columns contain the right singular vectors
\ei
\eb

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{SVD example in R}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# X matrix}
\hlkwd{set.seed}\hlstd{(}\hlnum{22}\hlstd{)}
\hlstd{X} \hlkwb{=} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(}\hlnum{20}\hlstd{),} \hlnum{5}\hlstd{,} \hlnum{4}\hlstd{)}

\hlcom{# singular value decomposition}
\hlstd{SVD} \hlkwb{=} \hlkwd{svd}\hlstd{(X)}

\hlcom{# elements returned by svd()}
\hlkwd{names}\hlstd{(SVD)}
\end{alltt}
\begin{verbatim}
## [1] "d" "u" "v"
\end{verbatim}
\begin{alltt}
\hlcom{# vector of singular values}
\hlstd{(d} \hlkwb{=} \hlstd{SVD}\hlopt{$}\hlstd{d)}
\end{alltt}
\begin{verbatim}
## [1] 3.9516353 2.0223602 1.4748193 0.4324292
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{SVD example in R (con't)}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# matrix of left singular vectors}
\hlstd{(U} \hlkwb{=} \hlstd{SVD}\hlopt{$}\hlstd{u)}
\end{alltt}
\begin{verbatim}
##            [,1]        [,2]       [,3]        [,4]
## [1,] -0.4251177 -0.53913435 -0.7232572  0.00979433
## [2,]  0.5268694 -0.76862769  0.2860048  0.05610045
## [3,]  0.5752546  0.04999546 -0.4421464  0.13107213
## [4,]  0.2215220  0.05272644 -0.1702161 -0.95123359
## [5,] -0.4021114 -0.33655016  0.4130778 -0.27337073
\end{verbatim}
\begin{alltt}
\hlcom{# matrix of right singular vectors}
\hlstd{(V} \hlkwb{=} \hlstd{SVD}\hlopt{$}\hlstd{v)}
\end{alltt}
\begin{verbatim}
##            [,1]       [,2]        [,3]       [,4]
## [1,]  0.5708354 -0.7406782  0.33862988  0.1042716
## [2,] -0.2741800 -0.5295008 -0.76797328  0.2338189
## [3,]  0.2772481  0.3206239 -0.04462207  0.9046229
## [4,]  0.7225689  0.2611992 -0.54180782 -0.3407543
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{SVD example in R (con't)}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# U orthonormal  (U'U = I)}
\hlkwd{t}\hlstd{(U)} \hlopt{%*%} \hlstd{U}
\end{alltt}
\begin{verbatim}
##              [,1]          [,2]          [,3]          [,4]
## [1,] 1.000000e+00  1.387779e-16  2.775558e-17  0.000000e+00
## [2,] 1.387779e-16  1.000000e+00 -2.775558e-17 -8.326673e-17
## [3,] 2.775558e-17 -2.775558e-17  1.000000e+00  5.551115e-17
## [4,] 0.000000e+00 -8.326673e-17  5.551115e-17  1.000000e+00
\end{verbatim}
\begin{alltt}
\hlcom{# V orthonormal  (V'V = I)}
\hlkwd{t}\hlstd{(V)} \hlopt{%*%} \hlstd{V}
\end{alltt}
\begin{verbatim}
##               [,1]          [,2]          [,3]          [,4]
## [1,]  1.000000e+00 -1.110223e-16 -5.551115e-17  1.110223e-16
## [2,] -1.110223e-16  1.000000e+00  8.326673e-17  1.942890e-16
## [3,] -5.551115e-17  8.326673e-17  1.000000e+00 -8.326673e-17
## [4,]  1.110223e-16  1.942890e-16 -8.326673e-17  1.000000e+00
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{SVD example in R (con't)}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# X equals U D V'}
\hlstd{U} \hlopt{%*%} \hlkwd{diag}\hlstd{(d)} \hlopt{%*%} \hlkwd{t}\hlstd{(V)}
\end{alltt}
\begin{verbatim}
##            [,1]        [,2]        [,3]       [,4]
## [1,] -0.5121391  1.85809239 -0.76390728 -0.9221536
## [2,]  2.4851837 -0.06602641  0.08196190  0.8615624
## [3,]  1.0078262 -0.16276495  0.74302828  2.0029422
## [4,]  0.2928146 -0.19986068 -0.08402219  0.9365510
## [5,] -0.2089594  0.30056173 -0.79289452 -1.6157349
\end{verbatim}
\begin{alltt}
\hlcom{# compare to X}
\hlstd{X}
\end{alltt}
\begin{verbatim}
##            [,1]        [,2]        [,3]       [,4]
## [1,] -0.5121391  1.85809239 -0.76390728 -0.9221536
## [2,]  2.4851837 -0.06602641  0.08196190  0.8615624
## [3,]  1.0078262 -0.16276495  0.74302828  2.0029422
## [4,]  0.2928146 -0.19986068 -0.08402219  0.9365510
## [5,] -0.2089594  0.30056173 -0.79289452 -1.6157349
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{SVD and Cross-products}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Data Matrix}

\begin{block}{Data}
The analyzed data can be expressed in matrix format $\mathbf{X}$:

\[ \underset{n \times p}{\mathbf{X}} = 
\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1p} \\
x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{np} \\
\end{array}\right]
\]
\end{block}

\begin{itemize}
 \item $n$ objects in the rows
 \item $p$ variables in the columns
\end{itemize}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Relation of SVD and Cross-Product Matrices}

The cross-product matrix of columns of $\mathbf{X}$ can be expressed as:

{\Large
$$
\mathbf{X^\mathsf{T}X = V D^2 V^\mathsf{T}}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Relation of SVD and Cross-Product Matrices}

The cross-product matrix of columns can be expressed as:

{\large
\begin{align*}
\mathbf{X^\mathsf{T} X} &= (\mathbf{U D V^\mathsf{T}})^\mathsf{T} (\mathbf{U D V^\mathsf{T}}) \\
&= (\mathbf{V D U^\mathsf{T}}) (\mathbf{U D V^\mathsf{T}}) \\
&= \mathbf{V D} (\mathbf{U^\mathsf{T}} \mathbf{U}) \mathbf{D V^\mathsf{T}} \\
&= \mathbf{V D^2 V^\mathsf{T}}
\end{align*}
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Relation of SVD and Cross-Product Matrices}

The cross-product matrix of rows of $\mathbf{X}$ can be expressed as:

{\Large
$$
\mathbf{XX^\mathsf{T} = U D^2 U^\mathsf{T}}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Relation of SVD and Cross-Product Matrices}

The cross-product matrix of rows can be expressed as:

{\large
\begin{align*}
\mathbf{X X^\mathsf{T}} &= (\mathbf{U D V^\mathsf{T}}) (\mathbf{U D V^\mathsf{T}})^\mathsf{T} \\
&= (\mathbf{U D V^\mathsf{T}}) (\mathbf{V D U^\mathsf{T}}) \\
&= \mathbf{U D} (\mathbf{V^\mathsf{T}} \mathbf{V}) \mathbf{D U^\mathsf{T}} \\
&= \mathbf{U D^2 U^\mathsf{T}}
\end{align*}
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Relation of SVD and Cross-Product Matrices}

One of the interesting things about SVD is that $\mathbf{U}$ and $\mathbf{V}$ are 
matrices whose columns are eigenvectors of product moment matrices that are 
\textit{derived} from $\mathbf{X}$. Specifically, 

\bi
  \item $\mathbf{U}$ is the matrix of eigenvectors of (symmetric) $\mathbf{XX^\mathsf{T}}$ of order $n \times n$
  \item $\mathbf{V}$ is the matrix of eigenvectors of (symmetric) $\mathbf{X^\mathsf{T}X}$ 
of oreder $p \times p$
\ei

Of additional interest is the fact that $\mathbf{D}$ is a diagonal matrix whose 
main diagonal entries are the square roots of $\mathbf{\Lambda}$, the \textit{common} 
matrix of eigenvalues of $\mathbf{XX^\mathsf{T}}$ and $\mathbf{X^\mathsf{T}X}$.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Relation between EVD and SVD}

The EVD of the cross-product matrix of columns (or minor product moment) 
$\mathbf{X^\mathsf{T} X}$ can be expressed as:

{\large
$$
\mathbf{X^\mathsf{T} X} = \mathbf{V \Lambda V^\mathsf{T}}
$$
}

in terms of the SVD factorization of $\mathbf{X}$:
{\large
$$
\mathbf{X^\mathsf{T} X} = \mathbf{V D^2 V^\mathsf{T}}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Relation between EVD and SVD}

The EVD of the cross-product matrix of rows (or major product moment)
$\mathbf{X X^\mathsf{T}}$ can be expressed as:

{\large
$$
\mathbf{X X^\mathsf{T}} = \mathbf{U \Lambda U^\mathsf{T}}
$$
}

in terms of the SVD factorization of $\mathbf{X}$:
{\large
$$
\mathbf{X X^\mathsf{T}} = \mathbf{U D^2 U^\mathsf{T}}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{Rank Reduction}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Rank-Reduction Theorem}

In terms of the diagonal elements $l_1, l_2, \dots, l_r$ of 
$\mathbf{D}$, the columns $\mathbf{u_1}, \dots, \mathbf{u_r}$ of 
$\mathbf{U}$, and the columns $\mathbf{v_1}, \dots, \mathbf{v_r}$ of 
$\mathbf{V}$, the basic structure of $\mathbf{X}$ may be written as

{\large
$$
\mathbf{X} = l_1 \mathbf{u_1 v^\mathsf{T}_1} + l_2 \mathbf{u_2 v^\mathsf{T}_2} 
+ \dots + l_p \mathbf{u_r v^\mathsf{T}_r}
$$
}

which shows that the matrix $\mathbf{X}$ of rank $p$ is a linear combination 
of $r$ matrices of rank 1.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Rank-Reduction Theorem}

A very interesting and alternative way to represent the SVD is with the 
following formula:

{\Large
$$
\mathbf{X} = \sum_{k=1}^{r} l_k \mathbf{u_k} \mathbf{v_{k}^{\mathsf{T}}}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Diagram}
\begin{center}
\ig[width=11cm]{images/svd-rank-one.pdf}

{\lolit SVD as sum of rank one matrices (assuming $r = p$)}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Rank-Reduction Theorem}

SVD alternative formula:

$$
\mathbf{X} = \sum_{k=1}^{r} l_k \mathbf{u_k} \mathbf{v_{k}^{\mathsf{T}}}
$$

\bi
  \item This expresses the SVD as a sum of $r$ rank-1 matrices.
  \item This result is formalized in what is known as the \textbf{SVD theorem} 
  described by Carl Eckart and Gale Young in 1936, and it is often referred to 
  as the \textit{Eckart-Young theorem}.
  \item This theorem applies to practically any arbitrary rectangular matrix. 
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Rank-Reduction Theorem}

\begin{center}
What if you take $m < r$ terms?

{\large
$$
\mathbf{\hat{X}} = \sum_{k=1}^{m} l_k \mathbf{u_k} \mathbf{v_{k}^{\mathsf{T}}}
$$
}

{\lit How would $\mathbf{\hat{X}}$ compare to $\mathbf{X}$?}
\end{center}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Rank-Reduction Theorem}

The SVD theorem of Eckart and Young is related to the important problem of 
approximating a matrix.

\bigskip
The basic result says that if $\mathbf{X}$ is an 
$n \times p$ rectangular matrix, then the best $r$-dimensional approximation 
$\hat{\mathbf{X}}$ to $\mathbf{X}$ is obtained by minimizing:

{\large
$$
min \quad \| \mathbf{X - \hat{X}} \|^2
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Rank-Reduction Theorem}

The minimization problem:

{\large
$$
min \quad \| \mathbf{X - \hat{X}} \|^2
$$
}

is a special type of approximation: a least squares approximation. 

\bigskip
The solution is obtained by taking the first $m$ elements of matrices 
$\mathbf{U, D, V}$ so that $\mathbf{\hat{X} = U_m D_m V^\mathsf{T}_m}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD rank-two approximation}
\begin{center}
\ig[width=11cm]{images/svd-rank-two.pdf}

{\lolit SVD as sum of two rank one matrices}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{SVD Rank-Reduction Theorem}

The best 2-rank approximation $\mathbf{\hat{X}}$ of $\mathbf{X}$ is given by:

{\large
$$
\mathbf{\hat{X}} = l_1 \mathbf{u_1 v^\mathsf{T}_1} + l_2 \mathbf{u_2 v^\mathsf{T}_2}
$$
}

We can say that the ``information'' contained in $n \times p$ values is compressed into 
$n \times 2$ values.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Bibliography}

{\footnotesize
\bi
  \item \textbf{Multivariate Analysis} by Maurice Tatsuoka (1988).
  \textit{Chapter 5: More Matrix Algebra}. Macmillan Publishing.
  \item \textbf{Mathematical Tools for Applied Multivariate Analysis} by J.D. Carroll, P.E. Green, and A. Chaturvedi (1997). 
  \textit{Chapter 5: Decomposition of Matrix Transformations: Eigenstructures and Quadratic Forms}. Academic Press. 
  \item \textbf{Principal Component Analysis} by Ian Jolliffe (2002). 
  \textit{Chapter 3, Section 3.5: The Singular Value Decomposition}. Springer.
  \item \textbf{Hands-on Matrix Algebra using R} by Hrishikesh Vinod (2011).
  \textit{Chapter 12: Kronecker Products and Singular Value Decomposition}. World Scientific.
\ei
}

\end{frame}

%------------------------------------------------

\end{document}
