\documentclass[12pt]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{tikz}

% make it pretty
\input{../header.tex}

%------------------------------------------------

\title{Eigenvalue Decomposition}
\subtitle{Matrix Algebra 4 Statistical Learning}
\author{\href{http://www.gastonsanchez.com}{Gaston Sanchez}}
\institute{\href{https://creativecommons.org/licenses/by-sa/4.0/}{\tt \scriptsize \color{foreground} CC BY-SA 4.0}}
\date{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}



% no page number in first slide
{
  \setbeamertemplate{footline}{} 
  \frame{\titlepage} 
}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{Matrix Decompositions}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{Decompositions}

Matrix decompositions, also known as matrix factorizations

{\Large
$$
\mathbf{M = A B} \qquad \text{or} \qquad \mathbf{M = A B C}
$$
}

are a means of expressing a matrix as a product 
of usually two or three {\hilit simpler} matrices.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Importance of Decompositions}

\bb{What for?}
Matrix decompositions make it easier to study the properties of matrices. Likewise, many computation tasks become easier with decompositions.

\bigskip
They play a relevant role in multivariate data analysis. Often, the solution
to many techniques are obtained (or derived) from a matrix decomposition. 
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Decompositions: What for?}

\bbi
  \item solving systems of linear equations
  \item inverting a matrix
  \item analyzing numerical stability of a system
  \item understanding the structure of data
  \item finding basis for column space (or row space) of a matrix
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Some Assumptions}

\bb{Real Matrices}
We will assume all matrices to be real matrices, i.e. matrices containing 
elements in the set of Real numbers.
\eb

\bb{Dimensions $n \geq p$}
Unless otherwise stated, we will also assume matrices with more rows than columns.
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Decompositions}

A matrix decomposition can be described by an equation:
$$
\mathbf{M} = \mathbf{A B C}
$$

where the dimensions of the matrices are as follows: 

\bbi
  \item $\mathbf{M}$ is $n \times p$ (assume $n > p$)
  \item $\mathbf{A}$ is $n \times k$ (usually $k < p$)
  \item $\mathbf{B}$ is $k \times k$ (usually diagonal)
  \item $\mathbf{C}$ is $k \times p$ 
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Matrix Decomposition}
\begin{center}
\ig[width=10cm]{images/matrix-decomposition.pdf}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Interpreting Decompositions}

The equation that describes a decomposition:
$$
\mathbf{M} = \mathbf{A B C}
$$

\bi
  \item does not explain how to compute one
  \item does not explain how such decomposition can reveal the
structures implicit in a data matrix.
  \item Seeing how a matrix decomposition reveals structure in a dataset is more 
complicated
  \item Each decomposition reveals a different kind of implicit structure
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Types of matrices}

\bb{Two types of matrices}
We concentrate on the two types of matrices important in statistics:
\eb

\bbi
 \item general \textbf{rectangular} matrices used to represent data tables.
 \item \textbf{positive semi-definite} matrices used to represent covariance matrices, 
correlation matrices, and any matrix that results from a crossproduct.
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Two Special Decompositions}

\bb{EVD and SVD}
There are many types of matrix decompositions but for now we are going to consider only two:
\bbi
 \item Eigen-Value Decomposition (EVD)
 \item Singular Value Decomposition (SVD)
\ei
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{EVD}

\bb{Eigenvalue Decomposition}
\bbi
  \item EVD applies to square matrices in general.
  \item A special type of square matrices are \textbf{symmetric} matrices.
  \item In data analysis methods, these matrices usually appear in the form of cross-product 
  association matrices: \\
  e.g. $\mathbf{X^\mathsf{T} X}$ and $\mathbf{X X^\mathsf{T}}$
  \item The attractive thing about EVD is that when applied to symmetric matrices the results have a ``simple'' nice structure.
  \ei
\eb

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigenvalue and Eigenvector}

Consider the matrix $\mathbf{A}$:

$$
\mathbf{A} =
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
$$

associated to the linear transformation $T(\mathbf{x})$ given by:

$$
T(\mathbf{x}) = 
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
\mathbf{x} = \mathbf{Ax}
$$

and assume vectors $\mathbf{v} = (2, 1)$ and $\mathbf{u} = (-1, 1)$

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigenvalue and Eigenvector}

$$
T(\mathbf{v}) = 
\
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
\
\begin{bmatrix} 
2 \\
1
\end{bmatrix}
\
=
\begin{bmatrix} 
4 \\
2
\end{bmatrix}
$$

$$
T(\mathbf{u}) = 
\
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
\
\begin{bmatrix} 
-1 \\
1
\end{bmatrix}
\
=
\begin{bmatrix} 
-5 \\
-1
\end{bmatrix}
$$

\begin{center}
\ig[width=7cm]{images/linear-transformation.png}

$\mathbf{u}$ is changing its direction, but not $\mathbf{v}$
\end{center}

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigenvalue and Eigenvector}

Given an $n \times n$ matrix $\mathbf{M}$, $\lambda$ is an \textbf{eigenvalue}
of $\mathbf{M}$ if there exists a non-trivial solution $\mathbf{v}$ of the
equation:

$$
\mathbf{M v} = \lambda \mathbf{v}
$$

The solution $\mathbf{v}$ is the \textbf{eigenvector} associated to the
eigenvalue $\lambda$

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigen-Value Decomposition}

\bb{EVD}
An $n \times n$ {\hilit symmetric matrix} $\mathbf{M}$ can be decomposed as:

$$
\mathbf{M = U \Lambda U^\mathsf{T}}
$$
\eb

\bb{where}
\bi
 \item $\mathbf{U}$ is a $n \times p$ column \textbf{orthonormal} matrix containing the eigen-vectors of $\mathbf{M}$
 \item $\mathbf{\Lambda}$ is a $p \times p$ \textbf{diagonal} matrix containing the eigen-values of $\mathbf{M}$
\ei
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{EVD}

$$
\mathbf{M = U \Lambda U^\mathsf{T}}
$$

$$
\mathbf{M} = 
\
\begin{bmatrix} 
u_{11} & \cdots & u_{1p} \\
u_{21} & \cdots & u_{2p} \\
\vdots & \ddots & \vdots \\
u_{n1} & \cdots & u_{np} \\
\end{bmatrix}
\
\begin{bmatrix} 
\lambda_1 & \cdots & 0 \\ 
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda_p \\
\end{bmatrix}
\
\begin{bmatrix} 
u_{11} & \cdots & u_{n1} \\
u_{12} & \cdots & u_{n2} \\
\vdots & \ddots & \vdots \\
u_{1p} & \cdots & u_{np} \\
\end{bmatrix}
$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

Vectors, which under a given transformation $\mathbf{M}$ map into 
themselves or multiples of themselves, are called invariant vectors under 
that transformation. It follows that such vectors satisfy the relation:

{\Large
$$
\mathbf{M x} = \lambda \mathbf{x}
$$
}

where $\lambda$ is a scalar.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

The matrix equation:

{\large
$$
\mathbf{M x} = \lambda \mathbf{x}
$$
}

can be rearranged as follows:
{\large
$$
\mathbf{M x} - \lambda \mathbf{x} = \mathbf{0}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

Given

{\large
$$
\mathbf{M x} - \lambda \mathbf{x} = \mathbf{0}
$$
}

We can factor out $\mathbf{x}$
{\large
$$
(\mathbf{M} - \lambda \mathbf{I}) \mathbf{x} = \mathbf{0}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

Obtaining the eigenstructure of a (square) matrix involves solving the 
\textbf{characteristic equation}

{\large
$$
det(\mathbf{M} - \lambda_i \mathbf{I}) = 0
$$
}

If $\mathbf{M}$ is of order $n \times n$, then we can obtain $n$ roots of the 
equation. These roots are called the \textbf{eigenvalues}.

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{EVD in R}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{\texttt{eigen()} in R}
\begin{block}{\texttt{eigen()} function}
R provides the function {\hilit \code{eigen()}} to perform an 
eigenvalue decomposition of a square matrix.
\end{block}

\bb{\texttt{eigen()} output}
A list with the following components
\bi
 \item {\mdlit \code{values}} a vector containing the eigenvalues
 \item {\mdlit \code{vectors}} a matrix whose columns contain the eigenvectors
\ei
\eb

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{EVD example in R}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# X'X matrix}
\hlkwd{set.seed}\hlstd{(}\hlnum{22}\hlstd{)}
\hlstd{X} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(USArrests)}
\hlstd{XtX} \hlkwb{<-} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X}

\hlcom{# eigenvalue decomposition}
\hlstd{EVD} \hlkwb{=} \hlkwd{eigen}\hlstd{(XtX)}

\hlcom{# elements returned by eigen()}
\hlkwd{names}\hlstd{(EVD)}
\end{alltt}
\begin{verbatim}
## [1] "values"  "vectors"
\end{verbatim}
\begin{alltt}
\hlcom{# vector of eigenvalues}
\hlstd{(lambdas} \hlkwb{=} \hlstd{EVD}\hlopt{$}\hlstd{values)}
\end{alltt}
\begin{verbatim}
## [1] 2013735.2431   37957.1103    2084.9578     326.5089
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{EVD example in R (con't)}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# matrix of eigenvectors}
\hlstd{(V} \hlkwb{<-} \hlstd{EVD}\hlopt{$}\hlstd{vectors)}
\end{alltt}
\begin{verbatim}
##             [,1]        [,2]        [,3]        [,4]
## [1,] -0.04239181  0.01616262  0.06588426  0.99679535
## [2,] -0.94395706  0.32068580 -0.06655170 -0.04094568
## [3,] -0.30842767 -0.93845891 -0.15496743  0.01234261
## [4,] -0.10963744 -0.12725666  0.98347101 -0.06760284
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{Properties of Matrix \\ Eigenstructures}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{1.}} The sum of the eigenvalues of a matrix $\mathbf{A}$ equals the 
sum of the main diagonal elements (i.e. the trace) of the matrix.

$$
\sum_{i = 1}^{n} \lambda_i = \sum_{i = 1}^{n} a_{ii}
$$

\bigskip

{\mdlit \textbf{2.}} The product of the eigenvalues of a matrix $\mathbf{A}$ equals the 
determinant of $\mathbf{A}$

$$
\prod_{i = 1}^{n} \lambda_i = | \mathbf{A} |
$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{3.}} If we have the matrix $\mathbf{B} = \mathbf{A} + k \mathbf{I}$, where $k$ is 
a scalar, then the eigenvectors of $\mathbf{B}$ are the same as those of 
$\mathbf{A}$, and the $i$-th eigenvalue of $\mathbf{B}$ is

$$
\lambda_i + k
$$

where $\lambda_i$ is the $i$-th eigenvalue of $\mathbf{A}$

\bigskip

{\mdlit \textbf{4.}} If we have the matrix $\mathbf{C} = k \mathbf{A}$, where $k$ is a scalar, then
$\mathbf{C}$ has the same eigenvectors as $\mathbf{A}$ and

$$
k \lambda_i
$$

is the eigenvalue of $\mathbf{C}$, where $\lambda_i$ is the $i$-th eigenvalue of 
$\mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{5.}} If we have the matrix $\mathbf{A}^{p}$, where $p$ is a 
positive integer, then $\mathbf{A}^{p}$ has the same 
eigenvectors as $\mathbf{A}$ and

$$
\lambda_{i}^{p}
$$

is the $i$-th eigenvalue of $\mathbf{A}^{p}$, where $\lambda_i$ is the $i$-th 
eigenvalue of $\mathbf{A}$

\bigskip

{\mdlit \textbf{6.}} If $\mathbf{A}^{-1}$ exists, then $\mathbf{A}^{-p}$ has the same 
eigenvectors as $\mathbf{A}$ and

$$
\lambda_{i}^{-p}
$$

is the $i$-th eigenvalue of $\mathbf{A}^{-p}$ corresponding to the $i$-th 
eigenvalue of $\mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{7.}} If a symmetric matrix $\mathbf{A}$ can be written as the
product

$$
\mathbf{A = UDU^\mathsf{T}}
$$

where $\mathbf{D}$ is a diagonal with all entries nonnegative and $\mathbf{U}$
is an orthogonal matrix of eigenvectors, then 

$$
\mathbf{A}^{1/2} = \mathbf{UD}^{1/2} \mathbf{U}^\mathsf{T}
$$

and it is the case that $\mathbf{A}^{1/2} \mathbf{A}^{1/2} = \mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{8.}} If a symmetric matrix $\mathbf{A}^{-1}$ can be written as the
product

$$
\mathbf{A}^{-1} = \mathbf{UD}^{-1} \mathbf{U}^\mathsf{T}
$$

where $\mathbf{D}^{-1}$ is a diagonal with all entries nonnegative and $\mathbf{U}$
is an orthogonal matrix of eigenvectors, then 

$$
\mathbf{A}^{-1/2} = \mathbf{UD}^{-1/2} \mathbf{U}^\mathsf{T}
$$

and it is the case that $\mathbf{A}^{-1/2} \mathbf{A}^{-1/2} = \mathbf{A}^{-1}$

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{Power Method}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{About the Power Method}

One of the basic procedures following a successive approximation 
approach is precisely the {\hilit Power Method}.

\bigskip
In its simplest form, the Power Method (PM) allows us to find \textbf{the largest} 
eigenvector and its corresponding eigenvalue.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{About the Power Method}

Choose an arbitrary vector $\mathbf{w_0}$ to which we will apply the symmetric matrix 
$\mathbf{S}$ repeatedly to form the following sequence:

\begin{align*}
 \mathbf{w_1} &= \mathbf{S w_0} \\
 \mathbf{w_2} &= \mathbf{S w_1 = S^2 w_0} \\
 \mathbf{w_3} &= \mathbf{S w_2 = S^3 w_0} \\
 \vdots \\
 \mathbf{w_k} &= \mathbf{S w_{k-1} = S^k w_0}
\end{align*}

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{Power Method: Example}

Consider a matrix $\mathbf{S}$
$$
\mathbf{S} =
\begin{bmatrix}
3 & 1 \\
1 & 2 \\
\end{bmatrix}
$$

\bigskip
and an initial vector $\mathbf{w_0}$
$$
\mathbf{w_0} =
\begin{bmatrix}
-0.4 \\
1.3 \\
\end{bmatrix}
$$

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/power-method-example-1} 

}



\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{About the Power Method}

\bbi
  \item In practice, we must rescale the obtained vector $\mathbf{w_k}$ at each step.
  \item The rescaling will allows us to judge whether the sequence is converging.
  \item After some iterations, the vector $\mathbf{w_{k-1}}$ and $\mathbf{w_k}$ will be very similar
  \item Assuming a reasonable scaling strategy, the sequence will usually converge 
  to the dominant eigenvector of $\mathbf{S}$.
\ei

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/power-method-rescale-1} 

}



\end{knitrout}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Dominant Eigenvalue}

The obtained vector is the dominant eigenvector. To get the 
corresponding eigenvalue we calculate the so-called \textbf{Rayleigh quotient}
given by:

{\Large
$$
\lambda = \frac{\mathbf{w_{k}^{\mathsf{T}} S w_k}}{\mathbf{w_{k}^{\mathsf{T}} w_k}}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Remarks}

Conditions for the power method to be succesfully used: 
\bbi
  \item The matrix must have a \textit{dominant} eigenvalue. 
  \item The starting vector $\mathbf{w_0}$ must be nonzero. 
  \item We need to scale each of the vectors $\mathbf{w_k}$ \\
  {\lolit otherwise the algorithm will ``explode''}
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{PM Pseudocode}

Let's consider a more detailed version of the PM algorithm:
\begin{enumerate}
  \item Start with an arbitraty initial vector $\mathbf{w}$
  \item Obtain product $\mathbf{\tilde{w} = S w}$
  \item Normalize $\mathbf{\tilde{w}}$
  $$
  \text{e.g.} \quad \mathbf{w} = \frac{\mathbf{\tilde{w}}}{\| \mathbf{\tilde{w}} \|_{p=2}}
  $$
  \item Compare $\mathbf{w}$ with its previous version
  \item Repeat steps 2 till 4 until convergence
\end{enumerate}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Why does the PM work?}

Assume that the matrix $\mathbf{S}$ has $p$ eigenvalues 
$\lambda_1, \lambda_2, \dots, \lambda_p$, and that they are ordered in 
decreasing way $|\lambda_1| > |\lambda_2| \geq \dots \geq |\lambda_p|$.

\bigskip
Note that the first eigenvalue is strictly greater than the second one. This is 
a very important assumption.

\bigskip
In the same way, we'll assume that the matrix $\mathbf{S}$ has $p$ linearly 
independent vectors $\mathbf{u_1}, \dots, \mathbf{u_p}$ ordered in such a way 
that $\mathbf{u_j}$ corresponds to $\lambda_j$. 

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Why does the PM work?}

The initial vector $\mathbf{w_0}$ may be expressed as a linear combination of 
$\mathbf{u_1}, \dots, \mathbf{u_p}$

$$
\mathbf{w_0} = a_1 \mathbf{u_1} + \dots + a_p \mathbf{u_p}
$$

At every step of the iterative process the vector $\mathbf{w_k}$ is given by:

$$
\mathbf{w_k} = a_1 \lambda_{1}^k \mathbf{u_1} + \dots + a_p \lambda_{p}^k \mathbf{u_p}
$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Why does the PM work?}

Since $\lambda_1$ is the dominant eigenvalue, the component in the direction of 
$\mathbf{u_1}$ becomes relatively greater than the other components as $k$ 
increases. If we knew $\lambda_1$ in advance, we could rescale at each step by 
dividing by it to get:

$$
\left(\frac{1}{\lambda_{1}^k}\right) \mathbf{w_k}= a_1 \mathbf{u_1} + \dots + a_p \left(\frac{\lambda_{p}^k}{\lambda_{1}^k}\right) \mathbf{u_p}
$$

which converges to the eigenvector $a_1 \mathbf{u_1}$, provided that $a_1$ is nonzero.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Why does the PM work?}

Of course, in real life this scaling strategy is not possible---we 
don't know $\lambda_1$. Consequenlty, the eigenvector is determined only up to 
a constant multiple, which is not a concern since the really important thing is 
the \textit{direction} not the length of the vector.

\bigskip
The speed of the convergence depends on how bigger $\lambda_1$ is respect with 
to $\lambda_2$, and on the choice of the initial vector $\mathbf{w_0}$. If 
$\lambda_1$ is not much larger than $\lambda_2$, then the convergence will be 
slow.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{More Remarks}

\bbi
  \item The power method is a sequential method.
  \item We can obtain $\mathbf{w_1, w_2}$, and so on, step by step.
  \item If we only need the first $k$ vectors, we can stop the procedure at the desired stage.
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Obtaining more eigenvectors?}

For \textbf{symmetric} matrices, once we've obtained the first eigenvector 
$\mathbf{w_1}$ and eigenvalue $\lambda_1$, we can compute the second eigenvector 
by reducing the matrix $\mathbf{S}$ by the 
amount explained by the first eigenvector. 

\bigskip
This operation of reduction is called \textbf{deflation} 
and the residual matrix is obtained as:

{\large
$$
\mathbf{S_1} = \mathbf{S} - \lambda_1 \mathbf{w_1 w_{1}^\mathsf{T}}
$$
}

To get the second eigenvalue and its corresponding eigenvector, 
we operate on $\mathbf{S_1}$ in the same way as the operations on $\mathbf{S}$.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{References}

{\footnotesize
\bi
  \item \textbf{Multivariate Analysis} by Maurice Tatsuoka (1988).
  \textit{Chapter 5: More Matrix Algebra}. Macmillan Publishing.
  \item \textbf{Mathematical Tools for Applied Multivariate Analysis} by J.D. Carroll, P.E. Green, and A. Chaturvedi (1997). 
  \textit{Chapter 5: Decomposition of Matrix Transformations: Eigenstructures and Quadratic Forms}. Academic Press. 
  \item \textbf{Hands-on Matrix Algebra using R} by Hrishikesh Vinod (2011).
  \textit{Chapter 9: Eigenvalues and Eigenvectors}. World Scientific.
\ei
}

\end{frame}

%------------------------------------------------

\end{document}
