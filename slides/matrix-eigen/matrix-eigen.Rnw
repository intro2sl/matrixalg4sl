\documentclass[12pt]{beamer}
\usepackage{tikz}

% make it pretty
\input{../header.tex}

%------------------------------------------------

\title{Eigenvalue Decomposition}
\subtitle{Matrix Algebra 4 Statistical Learning}
\author{\href{http://www.gastonsanchez.com}{Gaston Sanchez}}
\institute{\href{https://creativecommons.org/licenses/by-sa/4.0/}{\tt \scriptsize \color{foreground} CC BY-SA 4.0}}
\date{}

\begin{document}

<<setup, include=FALSE>>=
# smaller font size for chunks
opts_chunk$set(size = 'footnotesize')
options(width=87)
@

% no page number in first slide
{
  \setbeamertemplate{footline}{} 
  \frame{\titlepage} 
}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{Matrix Decompositions}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{Decompositions}

Matrix decompositions, also known as matrix factorizations

{\Large
$$
\mathbf{M = A B} \qquad \text{or} \qquad \mathbf{M = A B C}
$$
}

are a means of expressing a matrix as a product 
of usually two or three {\hilit simpler} matrices.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Importance of Decompositions}

\bb{What for?}
Matrix decompositions make it easier to study the properties of matrices. Likewise, many computation tasks become easier with decompositions.

\bigskip
They play a relevant role in multivariate data analysis. Often, the solution
to many techniques are obtained (or derived) from a matrix decomposition. 
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Decompositions: What for?}

\bbi
  \item solving systems of linear equations
  \item inverting a matrix
  \item analyzing numerical stability of a system
  \item understanding the structure of data
  \item finding basis for column space (or row space) of a matrix
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Some Assumptions}

\bb{Real Matrices}
We will assume all matrices to be real matrices, i.e. matrices containing 
elements in the set of Real numbers.
\eb

\bb{Dimensions $n \geq p$}
Unless otherwise stated, we will also assume matrices with more rows than columns.
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Decompositions}

A matrix decomposition can be described by an equation:
$$
\mathbf{M} = \mathbf{A B C}
$$

where the dimensions of the matrices are as follows: 

\bbi
  \item $\mathbf{M}$ is $n \times p$ (assume $n > p$)
  \item $\mathbf{A}$ is $n \times k$ (usually $k < p$)
  \item $\mathbf{B}$ is $k \times k$ (usually diagonal)
  \item $\mathbf{C}$ is $k \times p$ 
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Matrix Decomposition}
\begin{center}
\ig[width=10cm]{images/matrix-decomposition.pdf}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Interpreting Decompositions}

The equation that describes a decomposition:
$$
\mathbf{M} = \mathbf{A B C}
$$

\bi
  \item does not explain how to compute one
  \item does not explain how such decomposition can reveal the
structures implicit in a data matrix.
  \item Seeing how a matrix decomposition reveals structure in a dataset is more 
complicated
  \item Each decomposition reveals a different kind of implicit structure
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Types of matrices}

\bb{Two types of matrices}
We concentrate on the two types of matrices important in statistics:
\eb

\bbi
 \item general \textbf{rectangular} matrices used to represent data tables.
 \item \textbf{positive semi-definite} matrices used to represent covariance matrices, 
correlation matrices, and any matrix that results from a crossproduct.
\ei

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Two Special Decompositions}

\bb{EVD and SVD}
There are many types of matrix decompositions but for now we are going to consider only two:
\bbi
 \item Eigen-Value Decomposition (EVD)
 \item Singular Value Decomposition (SVD)
\ei
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{EVD}

\bb{Eigenvalue Decomposition}
\bbi
  \item EVD applies to square matrices in general.
  \item A special type of square matrices are \textbf{symmetric} matrices.
  \item In data analysis methods, these matrices usually appear in the form of cross-product 
  association matrices: \\
  e.g. $\mathbf{X^\mathsf{T} X}$ and $\mathbf{X X^\mathsf{T}}$
  \item The attractive thing about EVD is that when applied to symmetric matrices the results have a ``simple'' nice structure.
  \ei
\eb

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigenvalue and Eigenvector}

Consider the matrix $\mathbf{A}$:

$$
\mathbf{A} =
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
$$

associated to the linear transformation $T(\mathbf{x})$ given by:

$$
T(\mathbf{x}) = 
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
\mathbf{x} = \mathbf{Ax}
$$

and assume vectors $\mathbf{v} = (2, 1)$ and $\mathbf{u} = (-1, 1)$

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigenvalue and Eigenvector}

$$
T(\mathbf{v}) = 
\
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
\
\begin{bmatrix} 
2 \\
1
\end{bmatrix}
\
=
\begin{bmatrix} 
4 \\
2
\end{bmatrix}
$$

$$
T(\mathbf{u}) = 
\
\begin{bmatrix} 
3 & -2 \\
1 & 0
\end{bmatrix}
\
\begin{bmatrix} 
-1 \\
1
\end{bmatrix}
\
=
\begin{bmatrix} 
-5 \\
-1
\end{bmatrix}
$$

\begin{center}
\ig[width=7cm]{images/linear-transformation.png}

$\mathbf{u}$ is changing its direction, but not $\mathbf{v}$
\end{center}

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigenvalue and Eigenvector}

Given an $n \times n$ matrix $\mathbf{M}$, $\lambda$ is an \textbf{eigenvalue}
of $\mathbf{M}$ if there exists a non-trivial solution $\mathbf{v}$ of the
equation:

$$
\mathbf{M v} = \lambda \mathbf{v}
$$

The solution $\mathbf{v}$ is the \textbf{eigenvector} associated to the
eigenvalue $\lambda$

\end{frame}

%------------------------------------------------
  
\begin{frame}
\frametitle{Eigen-Value Decomposition}

\bb{EVD}
An $n \times n$ {\hilit symmetric matrix} $\mathbf{M}$ can be decomposed as:

$$
\mathbf{M = U \Lambda U^\mathsf{T}}
$$
\eb

\bb{where}
\bi
 \item $\mathbf{U}$ is a $n \times p$ column \textbf{orthonormal} matrix containing the eigen-vectors of $\mathbf{M}$
 \item $\mathbf{\Lambda}$ is a $p \times p$ \textbf{diagonal} matrix containing the eigen-values of $\mathbf{M}$
\ei
\eb

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{EVD}

$$
\mathbf{M = U \Lambda U^\mathsf{T}}
$$

$$
\mathbf{M} = 
\
\begin{bmatrix} 
u_{11} & \cdots & u_{1p} \\
u_{21} & \cdots & u_{2p} \\
\vdots & \ddots & \vdots \\
u_{n1} & \cdots & u_{np} \\
\end{bmatrix}
\
\begin{bmatrix} 
\lambda_1 & \cdots & 0 \\ 
\vdots & \ddots & \vdots \\
0 & \cdots & \lambda_p \\
\end{bmatrix}
\
\begin{bmatrix} 
u_{11} & \cdots & u_{n1} \\
u_{12} & \cdots & u_{n2} \\
\vdots & \ddots & \vdots \\
u_{1p} & \cdots & u_{np} \\
\end{bmatrix}
$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

Vectors, which under a given transformation $\mathbf{M}$ map into 
themselves or multiples of themselves, are called invariant vectors under 
that transformation. It follows that such vectors satisfy the relation:

{\Large
$$
\mathbf{M x} = \lambda \mathbf{x}
$$
}

where $\lambda$ is a scalar.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

The matrix equation:

{\large
$$
\mathbf{M x} = \lambda \mathbf{x}
$$
}

can be rearranged as follows:
{\large
$$
\mathbf{M x} - \lambda \mathbf{x} = \mathbf{0}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

Given

{\large
$$
\mathbf{M x} - \lambda \mathbf{x} = \mathbf{0}
$$
}

We can factor out $\mathbf{x}$
{\large
$$
(\mathbf{M} - \lambda \mathbf{I}) \mathbf{x} = \mathbf{0}
$$
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Eigenvectors}

Obtaining the eigenstructure of a (square) matrix involves solving the 
\textbf{characteristic equation}

{\large
$$
det(\mathbf{M} - \lambda_i \mathbf{I}) = 0
$$
}

If $\mathbf{M}$ is of order $n \times n$, then we can obtain $n$ roots of the 
equation. These roots are called the \textbf{eigenvalues}.

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{EVD in R}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{\texttt{eigen()} in R}
\begin{block}{\texttt{eigen()} function}
R provides the function {\hilit \code{eigen()}} to perform an 
eigenvalue decomposition of a square matrix.
\end{block}

\bb{\texttt{eigen()} output}
A list with the following components
\bi
 \item {\mdlit \code{values}} a vector containing the eigenvalues
 \item {\mdlit \code{vectors}} a matrix whose columns contain the eigenvectors
\ei
\eb

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{EVD example in R}

<<Xevd, tidy=FALSE, size='scriptsize'>>=
# X'X matrix
set.seed(22)
X <- as.matrix(USArrests)
XtX <- t(X) %*% X

# eigenvalue decomposition
EVD = eigen(XtX)

# elements returned by eigen()
names(EVD)

# vector of eigenvalues
(lambdas = EVD$values)
@

\end{frame}

%------------------------------------------------

\begin{frame}[fragile]
\frametitle{EVD example in R (con't)}

<<evd_V, tidy=FALSE, size='scriptsize'>>=
# matrix of eigenvectors
(V <- EVD$vectors)
@

\end{frame}

%------------------------------------------------

\begin{frame}
\begin{center}
\Huge{\hilit{Properties of Matrix \\ Eigenstructures}}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{1.}} The sum of the eigenvalues of a matrix $\mathbf{A}$ equals the 
sum of the main diagonal elements (i.e. the trace) of the matrix.

$$
\sum_{i = 1}^{n} \lambda_i = \sum_{i = 1}^{n} a_{ii}
$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{2.}} The product of the eigenvalues of a matrix $\mathbf{A}$ equals the 
determinant of $\mathbf{A}$

$$
\prod_{i = 1}^{n} \lambda_i = | \mathbf{A} |
$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{3.}} If we have the matrix $\mathbf{B} = \mathbf{A} + k \mathbf{I}$, where $k$ is 
a scalar, then the eigenvectors of $\mathbf{B}$ are the same as those of 
$\mathbf{A}$, and the $i$-th eigenvalue of $\mathbf{B}$ is

$$
\lambda_i + k
$$

where $\lambda_i$ is the $i$-th eigenvalue of $\mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{4.}} If we have the matrix $\mathbf{C} = k \mathbf{A}$, where $k$ is a scalar, then
$\mathbf{C}$ has the same eigenvectors as $\mathbf{A}$ and

$$
k \lambda_i
$$

is the eigenvalue of $\mathbf{C}$, where $\lambda_i$ is the $i$-th eigenvalue of 
$\mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{5.}} If we have the matrix $\mathbf{A}^{p}$, where $p$ is a 
positive integer, then $\mathbf{A}^{p}$ has the same 
eigenvectors as $\mathbf{A}$ and

$$
\lambda_{i}^{p}
$$

is the $i$-th eigenvalue of $\mathbf{A}^{p}$, where $\lambda_i$ is the $i$-th 
eigenvalue of $\mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{6.}} If $\mathbf{A}^{-1}$ exists, then $\mathbf{A}^{-p}$ has the same 
eigenvectors as $\mathbf{A}$ and

$$
\lambda_{i}^{-p}
$$

is the $i$-th eigenvalue of $\mathbf{A}^{-p}$ corresponding to the $i$-th 
eigenvalue of $\mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{7.}} If a symmetric matrix $\mathbf{A}$ can be written as the
product

$$
\mathbf{A = UDU^\mathsf{T}}
$$

where $\mathbf{D}$ is a diagonal with all entries nonnegative and $\mathbf{U}$
is an orthogonal matrix of eigenvectors, then 

$$
\mathbf{A}^{1/2} = \mathbf{UD}^{1/2} \mathbf{U}^\mathsf{T}
$$

and it is the case that $\mathbf{A}^{1/2} \mathbf{A}^{1/2} = \mathbf{A}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Properties of Eigenstructures}

{\mdlit \textbf{8.}} If a symmetric matrix $\mathbf{A}^{-1}$ can be written as the
product

$$
\mathbf{A}^{-1} = \mathbf{UD}^{-1} \mathbf{U}^\mathsf{T}
$$

where $\mathbf{D}^{-1}$ is a diagonal with all entries nonnegative and $\mathbf{U}$
is an orthogonal matrix of eigenvectors, then 

$$
\mathbf{A}^{-1/2} = \mathbf{UD}^{-1/2} \mathbf{U}^\mathsf{T}
$$

and it is the case that $\mathbf{A}^{-1/2} \mathbf{A}^{-1/2} = \mathbf{A}^{-1}$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Bibliography}

{\footnotesize
\bi
  \item \textbf{Multivariate Analysis} by Maurice Tatsuoka (1988).
  \textit{Chapter 5: More Matrix Algebra}. Macmillan Publishing.
  \item \textbf{Mathematical Tools for Applied Multivariate Analysis} by J.D. Carroll, P.E. Green, and A. Chaturvedi (1997). 
  \textit{Chapter 5: Decomposition of Matrix Transformations: Eigenstructures and Quadratic Forms}. Academic Press. 
  \item \textbf{Hands-on Matrix Algebra using R} by Hrishikesh Vinod (2011).
  \textit{Chapter 9: Eigenvalues and Eigenvectors}. World Scientific.
\ei
}

\end{frame}

%------------------------------------------------

\end{document}
